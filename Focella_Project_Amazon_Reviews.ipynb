{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This is my (Antonio Focella) attempt at Project 1: \"Finding similar items\"\n",
        "\n",
        "The task is to implement a detector of pairs of similar book reviews\n",
        "The idea behind this task is that the computational complexity of finding pairs of similar documents in domains like e-commerce requires algorithms that go beyond naive comparisons and can operate at scale.\n",
        "\n",
        "We need to identify pairs that are very similar in a scalable way. Checking every pair would be prohibitively slow (O(N^2) pairs).\n",
        "\n",
        "To overcome this limitation I will use the aforementioned techniques to compress each review into a compact signature that preserves similarity while using hashing to find likely pair without resorting to brute-force.\n",
        "\n",
        "As far as I know this solution is not based on one published on Kaggle or on other preexisting project, but I did follow the theory seen in the course as close as I could so I don't expect my project to be particularly original.\n",
        "\n",
        "I will mainly employ Shingling, MinHashing, and Locality-Sensitive Hashing (LSH). Such a pipeline allows to approximately reduce the problem from O(N^2) to O(N).\n",
        "\n",
        "This code takes about an hour to execute using 2 CPU cores.\n",
        "\n",
        "IMPORTANT: Remember to insert Kaggle credentials"
      ],
      "metadata": {
        "id": "XzsL8Aq4HDxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I start by installing the dependencies and Spark features I'll use for the project"
      ],
      "metadata": {
        "id": "_Y-bVP4zOmRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D8vkYN61F5KV"
      },
      "outputs": [],
      "source": [
        "#libraries\n",
        "\n",
        "!pip install -q pyspark findspark\n",
        "\n",
        "#standard imports\n",
        "import os\n",
        "from functools import reduce\n",
        "\n",
        "#Spark imports\n",
        "import findspark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, length, size\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import HashingTF, MinHashLSH\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.storagelevel import StorageLevel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick hardware check, it looks like I have two cores to work with."
      ],
      "metadata": {
        "id": "QBtY5OQ1OQl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the number of CPU cores available\n",
        "print(os.cpu_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtCAIzCTNTjp",
        "outputId": "52dff16f-49a3-40a1-a7a9-f00d0d61123a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yjSnxjP5ko9"
      },
      "source": [
        "We have a dataset of Amazon book reviews, so we are working with language and variable length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGrvy0Z836I-"
      },
      "source": [
        "First I Download the dataset from Kaggle using our user and key (write your own User and Key!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTH7ZrEc39tL",
        "outputId": "46e012b8-6c25-44ff-f715-c151048044a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content\n",
            " 99% 1.05G/1.06G [00:08<00:00, 331MB/s]\n",
            "100% 1.06G/1.06G [00:08<00:00, 131MB/s]\n"
          ]
        }
      ],
      "source": [
        "###-WRITE YOUR CREDENTIALS-###\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"Antonio Focella\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"arciduca\"\n",
        "\n",
        "###-WRITE YOUR CREDENTIALS-###\n",
        "\n",
        "#I use the credentials above to download the dataset from Kaggle\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBJlyF_aassF"
      },
      "source": [
        "I unzip and check what files are in the current working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8OQyAwAaK8j",
        "outputId": "0df60c02-9205-40aa-df03-fbdf3498140f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon-books-reviews.zip  sample_data\n",
            "amazon-books-reviews.zip  books_data.csv  Books_rating.csv  sample_data\n"
          ]
        }
      ],
      "source": [
        "#check data\n",
        "!ls\n",
        "\n",
        "#unzip dataset so spark can read it\n",
        "!unzip -q amazon-books-reviews.zip\n",
        "\n",
        "#confirm they have been extracted\n",
        "!ls\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVhk7X-4Lt7S"
      },
      "source": [
        "I will use spark for this project, as it allows to handle massive operations. Here I create a new session and make sure I use all CPU cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IAerKT456HG7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "0ac15241-5877-47bc-d357-a66a3f5dd14a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x77fd36c44b60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b8ce64df8e85:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#initialize findsaprk\n",
        "findspark.init()\n",
        "\n",
        "#stop already existing Spark sessions to avoid issues. If nothing defined, nothing to stop\n",
        "try:\n",
        "    spark.stop()\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "#create new Spark session, this should be configured to use all CPU cores (I think). Using 200 partitons as it's a big dataset\n",
        "#We simulate a distributed cluster, the Map and Reduce operations are parallelized across the cores\n",
        "#will need it for LSH which involves hashing millions of items independently\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .config(\"spark.driver.memory\", \"8g\") #6g better, 8g I think is the limit, I need 8 if I want to cache memory with 2^17 features but you might get kernel restarts\n",
        "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "         .getOrCreate())\n",
        "\n",
        "#I really want that my CPU remains fully utilized, but maybe should drop this\n",
        "cores = spark.sparkContext.defaultParallelism\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", str(cores * 4))\n",
        "\n",
        "#print session info\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPAWio_75jmt"
      },
      "source": [
        "Let's read the CSV into a Spark dataframe and check the data, I rename column id (which originally is book id and not review id, so a bit misleading) and create a new unique Id variable, less confusing.\n",
        "\n",
        "Here I take just a subsample of 1 million reviews to work faster with the code, which can however be scaled up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c1M8Igo7Sn9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01559932-b43b-4600-9278-5d13f9c01595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review: 1882931173: Jim of Oz \"jim-of-oz\"\n",
            "\n",
            "review: 0826414346: Kevin Killian\n",
            "\n",
            "review: 0826414346: John Granger\n",
            "\n",
            "review: 0826414346: Roy E. Perry \"amateur philosopher\"\n",
            "\n",
            "review: 0826414346: D. H. Richards \"ninthwavestore\"\n",
            "\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+---+\n",
            "|   book_id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text| id|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+---+\n",
            "|1882931173|Its Only Art If I...| NULL| AVCGYZL8FQQTD|Jim of Oz \"jim-of...|               7/7|         4.0|  940636800|Nice collection o...|This is only for ...|  0|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A30TK6U7DNS82R|       Kevin Killian|             10/10|         5.0| 1095724800|   Really Enjoyed It|I don't care much...|  1|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A3UH4UZ4RSVO82|        John Granger|             10/11|         5.0| 1078790400|Essential for eve...|If people become ...|  2|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A2MVUWT453QH61|Roy E. Perry \"ama...|               7/7|         4.0| 1090713600|Phlip Nel gives s...|Theodore Seuss Ge...|  3|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A22X4XUPKF66MR|D. H. Richards \"n...|               3/3|         4.0| 1107993600|Good academic ove...|Philip Nel - Dr. ...|  4|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+---+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "reviews_df = spark.read.csv(\"Books_rating.csv\", header=True, escape=\"\\\"\", quote=\"\\\"\", multiLine=True)\n",
        "\n",
        "#if too slow take just a fraction of reviews\n",
        "reviews_df = reviews_df.limit(1000000)\n",
        "\n",
        "#I rename the Id column and create a new one cause original dataset is confusing\n",
        "reviews_df = reviews_df.withColumnRenamed(\"Id\", \"book_id\")\n",
        "reviews_df = reviews_df.withColumn(\"id\", pyspark.sql.functions.monotonically_increasing_id())\n",
        "\n",
        "#print first rows\n",
        "for row in reviews_df.take(5):\n",
        "    print(f\"review: {row[0]}: {row[4]}\\n\")\n",
        "\n",
        "#also in tabluar form for quick human check\n",
        "reviews_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNcodQCIEIIe"
      },
      "source": [
        "I write a function to generate 10-shingles from a text string (I initially tried with k=5, which theory suggests for short documents like e-mails, but it appears to be too permissive as more unrelated documents share many shingles and it slows down the code).\n",
        "\n",
        "A \"shingle\" is a substring of length k found within the document. This transformation is crucial as it converts the \"similarity\" problem into a \"set intersection\" problem, which we can handle.\n",
        "\n",
        "I use sets so that each list has unique shingles, consistent with Jaccard's set definition. I then apply the function to our data. I also filter for empty reviews and for those shorter than the shingle length we chose.\n",
        "This solution appears to be fast enough and feasible.\n",
        "\n",
        "Notice that I am not normalizing whitespaces, I use the raw formatting to strictly detect near-duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wa9VLfgyADrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72982e2e-a8ef-47ce-8973-56fb8422272d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            shingles|\n",
            "+--------------------+\n",
            "|[ings by ol, ne b...|\n",
            "|[arhol, he , aden...|\n",
            "|[larity of , hild...|\n",
            "|[eir parent, eois...|\n",
            "|[(he starte, n of...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "#generate shingles of desired length from input text. If no text returns an empty list, also lowercases the text to make shingling case insensitive\n",
        "#10 should be large enough that random overlap is unlikely\n",
        "shin_len = 10 #I think 10 is the upper bound, more implies shingles become whole sentences\n",
        "\n",
        "def generate_shingles(text):\n",
        "  if text is None:\n",
        "    return []\n",
        "\n",
        "  s = text.lower() #force lowercase\n",
        "  shingles = set()\n",
        "\n",
        "  for i in range(len(s) - shin_len + 1):\n",
        "    shingles.add(s[i:i+shin_len]) #I \"slide a window\" of length k across the string\n",
        "\n",
        "  return list(shingles) #notice I am converting from a set to a list, removes duplicates\n",
        "\n",
        "#I create a UDF that applies the function above to a string column, returning the shingles\n",
        "shingle_udf = udf(generate_shingles, ArrayType(StringType()))\n",
        "\n",
        "#remove null & obviously too short text\n",
        "reviews_df = reviews_df.filter(col(\"review/text\").isNotNull()).filter(length(col(\"review/text\")) >= shin_len)\n",
        "\n",
        "#create a new column \"shingles\" by applying the shingling UDF to the reviews text\n",
        "reviews_df = reviews_df.withColumn(\"shingles\", shingle_udf(col(\"review/text\")))\n",
        "\n",
        "#display the first rows\n",
        "reviews_df.select(\"shingles\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALTERNATIVE\n",
        "\n",
        "I tried this alternative in Spark without a Python UDF but it appears to be much slower. I keep it as legacy code (I am not sure why it's slower)."
      ],
      "metadata": {
        "id": "jfBPdy02krtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#other necessary imports\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import re\n",
        "# from itertools import combinations\n",
        "# from tqdm import tqdm\n",
        "# import random\n",
        "\n",
        "# #generate shingles of desired length from input text. If no text returns an empty list, also lowercases the text to make shingling case insensitive\n",
        "# shin_len = 10\n",
        "\n",
        "# #generate shingles of desired length from input text. If no text returns an empty list, also lowercases the text to make shingling case insensitive\n",
        "# text_col = pyspark.sql.functions.col(\"`review/text`\") #'' needed because the column name has \"/\"\n",
        "# s = pyspark.sql.functions.lower(text_col)\n",
        "# n = pyspark.sql.functions.length(s)\n",
        "\n",
        "# #build a list of starting positions for each possible k-character substring\n",
        "# positions = pyspark.sql.functions.sequence(pyspark.sql.functions.lit(1), n - pyspark.sql.functions.lit(shin_len) + pyspark.sql.functions.lit(1))\n",
        "\n",
        "# #remove duplicates\n",
        "# shingles = pyspark.sql.functions.transform(positions, lambda i: pyspark.sql.functions.substring(s, i, shin_len))\n",
        "\n",
        "# #prepare an empty array of strings, it darws from this if the text is too short\n",
        "# empty_arr = pyspark.sql.functions.expr(\"array()\").cast(\"array<string>\")\n",
        "\n",
        "# #if text is long enough, store it\n",
        "# reviews_df = reviews_df.withColumn(\"shingles\", pyspark.sql.functions.when(n >= shin_len, shingles).otherwise(empty_arr))\n",
        "\n",
        "# reviews_df.select(\"shingles\").show(5, truncate=False)"
      ],
      "metadata": {
        "id": "S_X-vcgSifju"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7-tYqTpJj25"
      },
      "source": [
        "The set of all possible k-shingles in the English language is immense, so I must reduce the dimesnion of the \"space\" I work with.\n",
        "\n",
        "The next step is thus to create a MinHash signature for each set.\n",
        "MinHashing is a technique to compress a set into a small signature (i.e. a vector of integers) and simulate randomness instead of actual permutations.\n",
        "The key property of MinHash is that the fraction of components in which two signatures \"agree\" provides us with an unbiased estimate of Jaccard similarity.\n",
        "\n",
        "Spark has a built-in MinHash functionality to handle this efficiently, but before I need to convert each review's shingle set into a binary fearue vector, which I do using hashingTF (with \"binary\" option on as to adhere to what we saw in the course and to reduce work/memory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "25hFUxOuCFCu"
      },
      "outputs": [],
      "source": [
        "#I use HashingTF to convert shingles to a sparse binary feature vectors\n",
        "\n",
        "num_dim = 1<<20 #this is 2^20 features. The average number of shingles is about 1000 so should be ok, low enough collisions. If you want to chace you have to decrease (in my case to 2^18 using 8g memory)\n",
        "\n",
        "#each shingle is hashed into an index in [0,numFeatures] in the feature vector\n",
        "hashingTF = HashingTF(inputCol=\"shingles\", outputCol=\"features\", numFeatures=num_dim, binary=True) #here important to set binary=True\n",
        "\n",
        "#I filter out rows where the 'shingles' array is empty (This prevents exceptions in MinHashLSH for empty feature vectors) before transforming\n",
        "#I also keep only the columns needed downstream to speed up the code\n",
        "features_df = hashingTF.transform(reviews_df.filter(size(col(\"shingles\")) > 0)).select(\"id\",\"features\")\n",
        "\n",
        "#I store and materialize the cache (because features_df is reused)\n",
        "#features_df = features_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "#materialize_cache = features_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfAwJLbEOXUi"
      },
      "source": [
        "As expected each vector is very sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM_d5clSPtH8"
      },
      "source": [
        "The MinHash LSH model hashes each vector into a smaller signature and use LSH to find candidate pairs that are likely to have Jaccard distance below a certain threshold.\n",
        "\n",
        "The mh.fit model \"trains\" the LSH model on the data, computing the MinHash signature for each feature vector and preparing the hash tabels, while transform calculates the signatures by taking the (sparse) feature vector and applying the n hash functions.\n",
        "\n",
        "Every book review is now represented by an array of n integers.\n",
        "\n",
        "Since I want a similarity of 0.9 and we know that the \"threshold\" of banding is rougnly (1/b)^(1/r), I set the number of Hash Tables accordingly (I use low values for b and r as I need a very efficient code having just 2 CPU cores)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Qa4wqTMSOb_C"
      },
      "outputs": [],
      "source": [
        "#signature length. If we want to be more precise we can increase the number of Hash Tables, but that would slow us down\n",
        "#I experimented a bit and noticed I don't need too long signature length to get accurate results, and using low values improves speed a lot\n",
        "#as the error should be proportional to 1/sqrt(n), I think using about 100 minhash values is a bit low but ok (if, say, true Jaccard similarity is 0.5, StD is 0.05)\n",
        "b = 6 #number of bands\n",
        "r = 16 #rows per band\n",
        "n_sig = b*r #total number of minhash values per row (so signature length I use at this stage)\n",
        "\n",
        "#this is necessary to avoid the code becoming too slow\n",
        "\n",
        "#I use MinHash LSH model to simulate permutations, adding the output column for hash keys\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=n_sig)\n",
        "model = mh.fit(features_df) #this step prepares the random coefficients\n",
        "\n",
        "#I compute the MinHash signature for the features (hashes is an array of 1D vectors so convert to an array for later)\n",
        "sig = model.transform(features_df).select(\"id\", \"hashes\").select(\"id\", pyspark.sql.functions.transform(\"hashes\", lambda v: vector_to_array(v)[0].cast(\"long\")).alias(\"sig\")).persist(StorageLevel.MEMORY_AND_DISK)\n",
        "materialize_cache_2 = sig.count()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We still have the problem of finding pairs. To focus on pairs likely to be similar I use Locality-Sensitive Hashing (LSH).\n",
        "\n",
        "The idea is to divide the signature matrix into bands, and for each band to hash the portion of the signature to a bucket. Two reviews are a candidate pair if they hash to the same bucket in at least one band.\n",
        "\n",
        "LSH is not perfect as it might get false positives and false negatives.\n",
        "\n",
        "BandKey is a hash that represents the content of the band. if two reviews have identical values in one band they will end up in the same bucket for that band.\n",
        "\n",
        "I set a filter for too large buckets in order to improve performance (if a bucket has k items it might generate k^2 pairings)."
      ],
      "metadata": {
        "id": "jiAQtZAJyh6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split signatures into b bands of size r, hash each band to a bandKey\n",
        "#I basically take sig, convert into a single string, slice it, use xxhash64 to hash, and this is my \"bucket id\" for that band\n",
        "band_rows = []\n",
        "for band in range(b):\n",
        "    start = band * r + 1  #if band=0 start is 1 (Spark slice is 1 based not 0 based it appears...), if band=1 start is r+1 ecc\n",
        "    band_rows.append(\n",
        "        sig.select(\n",
        "            \"id\",\n",
        "            pyspark.sql.functions.lit(band).alias(\"band\"), #add column to store the band index\n",
        "            pyspark.sql.functions.xxhash64(pyspark.sql.functions.concat_ws(\"_\", pyspark.sql.functions.slice(\"sig\", start, r))).alias(\"bandKey\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "#unite all rows (id, band, bandKey), each review appears b times (once per band) with a different bandKey each time\n",
        "#then repartition (re-shuffle) so that rows with the same band & bandKey end up together, next operations should be faster\n",
        "buckets = reduce(lambda a, d: a.unionByName(d), band_rows).repartition(spark.sparkContext.defaultParallelism * 4, \"band\", \"bandKey\")\n",
        "\n",
        "\n",
        "#candidate pairs are those sharing a bandKey in any band. Since a few large buckets can slow down the code a lot, I add a bucket-size filter to drop huge buckets\n",
        "bucket_sizes = buckets.groupBy(\"band\", \"bandKey\").count() #count how many reviews are in each bucket\n",
        "MAX_BUCKET = 200  #lower = faster, but we may miss some true matches. If more than 200 I drop it\n",
        "buckets_small = (buckets.join(bucket_sizes.filter(pyspark.sql.functions.col(\"count\") <= MAX_BUCKET), on=[\"band\", \"bandKey\"],how=\"inner\"))\n",
        "buckets=buckets_small #here I substitute if I want to speed up the code"
      ],
      "metadata": {
        "id": "JHpf8minyliY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJWpbse1vCje"
      },
      "source": [
        "I now remove pairs where idA < idB to remove duplicates and self-matches and keep only couples where Jaccard similarity is above the chosen threshold, and I check a sample of results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate candidate pairs: join the table with itself is same band and bandKey, remove self-pairs & duplicates\n",
        "cand = (buckets.alias(\"a\")\n",
        "        .join(buckets.alias(\"b\"), on=[\"band\", \"bandKey\"])\n",
        "        .where(pyspark.sql.functions.col(\"a.id\") < pyspark.sql.functions.col(\"b.id\")) #remove duplicate\n",
        "        .select(pyspark.sql.functions.col(\"a.id\").alias(\"idA\"), pyspark.sql.functions.col(\"b.id\").alias(\"idB\")) #keep only id of the pairs\n",
        "        .distinct()) #want to make sure we don't have duplicates"
      ],
      "metadata": {
        "id": "D_mmWlEVJDW1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "11jd4h3giGak"
      },
      "outputs": [],
      "source": [
        "#join back the corresponding signatures so we can estimate their Jaccard similarity by comparing signature components\n",
        "#I select the useful output columns: first id, second id, first MinHash signature, second MinHash signature\n",
        "pairs = (cand\n",
        "         .join(sig.alias(\"sa\"), cand.idA == pyspark.sql.functions.col(\"sa.id\"))\n",
        "         .join(sig.alias(\"sb\"), cand.idB == pyspark.sql.functions.col(\"sb.id\"))\n",
        "         .select(\"idA\", \"idB\", pyspark.sql.functions.col(\"sa.sig\").alias(\"sigA\"), pyspark.sql.functions.col(\"sb.sig\").alias(\"sigB\")))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected fraction of equal signature rows is the estimate of Jaccard similarity."
      ],
      "metadata": {
        "id": "Sa-RLu2DtJ1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here we compute a MinHash based estimate of Jaccard similarity between the two reviews in each candidate pair\n",
        "pairs = pairs.withColumn(\n",
        "    \"jacc_est\", #create new column\n",
        "    (pyspark.sql.functions.aggregate( #compare sigA and sigB element-by element and produce an array of 1/0\n",
        "        pyspark.sql.functions.zip_with(\"sigA\", \"sigB\", lambda x, y: pyspark.sql.functions.when(x == y, pyspark.sql.functions.lit(1)).otherwise(pyspark.sql.functions.lit(0))),\n",
        "        pyspark.sql.functions.lit(0), #start from 0\n",
        "        lambda acc, x: acc + x #accunulate the ones\n",
        "    ) / pyspark.sql.functions.size(pyspark.sql.functions.col(\"sigA\")) #divide by signature length\n",
        "    )\n",
        "  )"
      ],
      "metadata": {
        "id": "uHn4a_sWEWQ-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I keep only pairs whose estimated Jaccard similarity is above the chosen threshold.\n",
        "\n",
        "We have achiedeved the near-duplicate detection goal."
      ],
      "metadata": {
        "id": "Fgsl8Vu3OnZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold=0.9 #change this if you want\n",
        "\n",
        "similar = pairs.where(pyspark.sql.functions.col(\"jacc_est\") >= threshold).select(\"idA\", \"idB\", \"jacc_est\")"
      ],
      "metadata": {
        "id": "XgL3TSqwEXvt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#quick check\n",
        "similar.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "F5gAoLsLEYbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a25d6e-16b7-4db4-d067-0784c8e513d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------------+\n",
            "|idA |idB |jacc_est          |\n",
            "+----+----+------------------+\n",
            "|262 |267 |1.0               |\n",
            "|2761|2766|1.0               |\n",
            "|3600|3608|1.0               |\n",
            "|4427|4439|1.0               |\n",
            "|4790|4796|1.0               |\n",
            "|5891|5892|0.9479166666666666|\n",
            "|6250|6514|1.0               |\n",
            "|4293|6636|1.0               |\n",
            "|4299|6642|1.0               |\n",
            "|6922|6973|1.0               |\n",
            "+----+----+------------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQCvjksYKdMz"
      },
      "source": [
        "Coalesce allows to get a single CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zZE0hKa33z3V"
      },
      "outputs": [],
      "source": [
        "#similar.write.mode(\"overwrite\").option(\"header\", True).csv(\"similar_pairs_out\")\n",
        "#similar.coalesce(1).write.csv(\"similar_review_pairs.csv\", header=True, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If needed we can check whether specific reviews are actually similar."
      ],
      "metadata": {
        "id": "X2VUsBXAUfms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#If needed, just to check some reviews are actually the same\n",
        "selected = reviews_df.where(\"id IN (262, 267)\")\n",
        "selected.show(truncate=False)"
      ],
      "metadata": {
        "id": "YC8ksVXYUP6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f1aafb-4cfd-451c-814a-daac2e1c3f7f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------------------------------+-----+--------------+----------------+------------------+------------+-----------+---------------------------------+--------------------------------------------------------------------------------------------------+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|book_id   |Title                                       |Price|User_id       |profileName     |review/helpfulness|review/score|review/time|review/summary                   |review/text                                                                                       |id |shingles                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "+----------+--------------------------------------------+-----+--------------+----------------+------------------+------------+-----------+---------------------------------+--------------------------------------------------------------------------------------------------+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|9687968478|Cien Aos de BOXEO (One Hundred Years of Box)|NULL |NULL          |NULL            |3/3               |5.0         |1050796800 |SI TE ATRAEN LAS HISTORIAS REALES|DE LUCHA Y TRIUNFO..ESTE LIBRO TE DEJAR&Aacute; MUY SATISFECHO...Aunque no seas aficionado al box!|262|[ triunfo.., echo...aun, ficionado , uy satisfe, unque no s, ucha y tri, libro te d, &aacute; m, y satisfec, ejar&aacut, de lucha y, ar&aacute;, do al box!, ha y triun, ho...aunqu, aficionado, o seas afi, te; muy sa, cute; muy , cionado al,  seas afic, eas aficio,  libro te , ibro te de, tisfecho.., este libro, .este libr, fo..este l,  dejar&aac, satisfecho, bro te dej, ste libro , aunque no , fecho...au, atisfecho., o te dejar, seas afici,  lucha y t, e libro te, e dejar&aa, jar&aacute, ..aunque n, ro te deja, iunfo..est, s aficiona, unfo..este,  aficionad, dejar&aacu,  satisfech,  muy satis, a y triunf,  y triunfo, ...aunque , r&aacute; , ute; muy s, muy satisf, sfecho...a, y triunfo., triunfo..e,  te dejar&, cho...aunq, e lucha y , ionado al , acute; muy, onado al b, riunfo..es, e; muy sat, o...aunque, o..este li,  no seas a, lucha y tr, te libro t, te dejar&a, que no sea, nado al bo, ado al box, ue no seas, nque no se, ; muy sati, isfecho..., no seas af, icionado a, .aunque no, e no seas , as aficion, ..este lib, nfo..este , cha y triu, aacute; mu]|\n",
            "|9687968478|Cien Aos de BOXEO (One Hundred Years of Box)|NULL |A2SZGIUKIIVDMS|Jennifer Jhonson|0/0               |5.0         |1128988800 |SI TE ATRAEN LAS HISTORIAS REALES|DE LUCHA Y TRIUNFO..ESTE LIBRO TE DEJAR&Aacute; MUY SATISFECHO...Aunque no seas aficionado al box!|267|[ triunfo.., echo...aun, ficionado , uy satisfe, unque no s, ucha y tri, libro te d, &aacute; m, y satisfec, ejar&aacut, de lucha y, ar&aacute;, do al box!, ha y triun, ho...aunqu, aficionado, o seas afi, te; muy sa, cute; muy , cionado al,  seas afic, eas aficio,  libro te , ibro te de, tisfecho.., este libro, .este libr, fo..este l,  dejar&aac, satisfecho, bro te dej, ste libro , aunque no , fecho...au, atisfecho., o te dejar, seas afici,  lucha y t, e libro te, e dejar&aa, jar&aacute, ..aunque n, ro te deja, iunfo..est, s aficiona, unfo..este,  aficionad, dejar&aacu,  satisfech,  muy satis, a y triunf,  y triunfo, ...aunque , r&aacute; , ute; muy s, muy satisf, sfecho...a, y triunfo., triunfo..e,  te dejar&, cho...aunq, e lucha y , ionado al , acute; muy, onado al b, riunfo..es, e; muy sat, o...aunque, o..este li,  no seas a, lucha y tr, te libro t, te dejar&a, que no sea, nado al bo, ado al box, ue no seas, nque no se, ; muy sati, isfecho..., no seas af, icionado a, .aunque no, e no seas , as aficion, ..este lib, nfo..este , cha y triu, aacute; mu]|\n",
            "+----------+--------------------------------------------+-----+--------------+----------------+------------------+------------+-----------+---------------------------------+--------------------------------------------------------------------------------------------------+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ukYGg0yMhbA"
      },
      "source": [
        "A quick human check confirms that the pairs are indeed very similar.\n",
        "The solution scales to large datasets because shingling, hashing, and joining are done in a distributed manner by Spark across the cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I declare that this material, which I now submit for assessment, is entirely my own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my work, and including any code produced using generative AI systems. I/ understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me or any other person for assessment on this or any other course of study"
      ],
      "metadata": {
        "id": "VFLAUdmwHXi4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}